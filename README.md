# Speak and Translate - AI-Powered Translation App

<div align="center">
  A multilingual voice translation assistant with natural speech synthesis
</div>

## ğŸŒ Languages | Sprachen

- [English](#english)
- [Deutsch](#deutsch)

---

<a id="english"></a>

# ğŸ—£ï¸ Speak and Translate - AI Translation Assistant [English]

A cross-platform application built with Flutter and Azure AI services that provides real-time speech translation, voice commands, and natural speech synthesis. The app supports conversations between multiple languages with rich educational features for language learners.

## ğŸ“‹ Features

- **ğŸ™ï¸ Voice Commands**: Activate microphone with "Jarvis" and stop recording with "Alexa"
- **ğŸ”Š Speech Recognition**: Convert spoken language to text with high accuracy
- **ğŸŒ Real-time Translation**: Translate between languages with AI-powered accuracy
- **ğŸ—£ï¸ Natural Speech Synthesis**: Hear translations in natural-sounding voices
- **ğŸ§  Educational Translations**: View word-by-word translations for language learning
- **ğŸ­ Multiple Translation Styles**: Access native, colloquial, informal, and formal translations
- **ğŸ’¬ Chat Interface**: Track conversation history with visual speech bubbles
- **ğŸ›ï¸ Hands-Free Mode**: Automatically play translations for continuous conversation
- **ğŸ”„ Bilingual Audio Playback**: Hear source and translated words sequentially
- **ğŸ“± Cross-Platform**: Works on Android, iOS, and web platforms

### Translation Capabilities In Detail

The application provides advanced translation options with four different formality levels:

1. **Native**: Professional translation as a native speaker would express it
2. **Colloquial**: Casual, everyday language with idioms and expressions
3. **Informal**: Relaxed language for casual situations
4. **Formal**: Professional language for business or formal contexts

Each translation includes word-by-word explanations to help users understand the structure and vocabulary of different languages, making this application particularly valuable for language learners.

## ğŸ”§ Technical Implementation

### Architecture Overview

The application consists of two main components:

1. **Flutter Mobile Client**:
   - UI for user interaction and chat interface
   - Handles audio recording and playback
   - Manages conversation history

2. **Python Backend Server**:
   - FastAPI REST service for translation and speech processing
   - Integrates with Google's Gemini AI for advanced translation
   - Uses Azure Cognitive Services for speech synthesis and recognition
   - Containerized with Docker for easy deployment

### Technologies Used

#### Mobile App
- Flutter with Dart
- Riverpod for state management
- Audio recording and playback libraries
- Material Design UI components

#### Backend Server
- Python 3.9
- FastAPI web framework
- Azure Cognitive Services Speech SDK
- Google's Gemini 2.0 AI model
- Docker for containerization
- Pydub for audio processing

#### DevOps
- GitHub Actions for CI/CD
- Azure Container Apps for hosting
- Docker Hub for container registry

### Code Organization

The codebase is structured following clean architecture principles:

```
â”œâ”€â”€ .github/workflows/      # CI/CD configuration for Azure deployment
â”œâ”€â”€ lib/                    # Flutter application source code
â”‚   â”œâ”€â”€ features/           # Feature modules (translation, voice, etc.)
â”‚   â”‚   â””â”€â”€ translation/    # Translation feature
â”‚   â”‚       â”œâ”€â”€ data/       # Data layer with models and repositories
â”‚   â”‚       â”œâ”€â”€ domain/     # Domain layer with entities and use cases
â”‚   â”‚       â””â”€â”€ presentation/ # UI layer with screens and providers
â”‚   â””â”€â”€ main.dart           # Flutter application entry point
â””â”€â”€ server/                 # Backend server
    â”œâ”€â”€ app/                # Server application code
    â”‚   â”œâ”€â”€ application/    # Application services
    â”‚   â”œâ”€â”€ domain/         # Domain entities
    â”‚   â””â”€â”€ infrastructure/ # API routes and external interfaces
    â”œâ”€â”€ Dockerfile          # Docker configuration for server
    â””â”€â”€ requirements.txt    # Python dependencies
```

## ğŸš€ Getting Started

### Prerequisites

Before getting started, you'll need to:

1. Install Flutter SDK (latest version)
2. Install Docker (for backend)
3. Obtain the following API credentials:
   - **Azure Speech Services**: Create an account at [Azure Portal](https://portal.azure.com), create a Speech service resource, and get your API key and region
   - **Google Gemini API**: Sign up at [Google AI Studio](https://makersuite.google.com/app/apikey) to get your Gemini API key
   - **Picovoice API**: Create an account at [Picovoice Console](https://console.picovoice.ai/) to get your API key for wake word detection

### Creating Your Project From Scratch

1. Create a new directory for your project:
```bash
# For Windows
mkdir speak_and_translate_app
cd speak_and_translate_app

# For macOS/Linux
mkdir speak_and_translate_app
cd speak_and_translate_app
```

2. Create a new Flutter project:
```bash
flutter create speak_and_translate
cd speak_and_translate
```

3. Replace the default Flutter project structure with the repository structure:
   - Replace the contents of the `lib` folder with the repository's `lib` folder
   - Create a `server` folder and add the backend files

### Server Setup

1. Create a `server` directory in your project root and navigate to it:
```bash
mkdir server
cd server
```

2. Copy the server files from the repository or create the necessary files manually following the repository structure.

3. Set up environment variables:
   Create a `.env` file in the server directory with your own API credentials:
```
AZURE_SPEECH_KEY=your_azure_speech_key
AZURE_SPEECH_REGION=your_azure_region
GEMINI_API_KEY=your_gemini_api_key
PICOVOICE_API_KEY=your_picovoice_api_key
TTS_DEVICE=cpu
```

4. Run with Docker:
```bash
docker-compose up -d
```

5. Or run locally with Python:
```bash
pip install -r requirements.txt
python -m app.main
```

### Mobile App Setup

1. Navigate back to the mobile app directory:
```bash
cd ..
```

2. Update the `pubspec.yaml` file with the required dependencies:
```yaml
dependencies:
  flutter:
    sdk: flutter
  cupertino_icons: ^1.0.8
  flutter_riverpod: ^2.6.1
  http: ^1.2.2
  json_annotation: ^4.9.0
  path_provider: ^2.1.5
  record: ^5.2.0
  permission_handler: ^11.3.1
  record_platform_interface: ^1.2.0
  flutter_sound: ^9.19.1
  just_audio: ^0.9.43
  flutter_markdown: ^0.7.5
  flutter_tts: ^4.2.1
  picovoice_flutter: ^3.0.4
  speech_to_text: ^7.0.0
```

3. Install dependencies:
```bash
flutter pub get
```

4. Configure API endpoint:
   In `lib/features/translation/data/repositories/translation_repository_impl.dart`, update the `baseUrl` to point to your server:
```dart
// Change this line to point to your server
final String baseUrl = 'http://localhost:8000'; 
```

5. Set up permissions:
   - For Android: Add microphone permissions to `android/app/src/main/AndroidManifest.xml`:
   ```xml
   <uses-permission android:name="android.permission.RECORD_AUDIO"/>
   <uses-permission android:name="android.permission.INTERNET"/>
   ```
   
   - For iOS: Add microphone permissions to `ios/Runner/Info.plist`:
   ```xml
   <key>NSMicrophoneUsageDescription</key>
   <string>This app needs microphone access for speech recognition</string>
   ```

6. Run the app:
```bash
flutter run
```

## ğŸ“¦ Deployment

### Obtaining API Credentials

Before deployment, you need to obtain your own API credentials:

1. **Azure Speech Services**:
   - Go to [Azure Portal](https://portal.azure.com)
   - Create a new resource â†’ AI + Machine Learning â†’ Speech Service
   - After creation, go to "Keys and Endpoint" to find your key and region

2. **Google Gemini API**:
   - Visit [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Create an API key for the Gemini model

3. **Picovoice API**:
   - Create an account at [Picovoice Console](https://console.picovoice.ai/)
   - Generate a new access key for wake word detection

### Server Deployment with Azure Container Apps

The backend can be deployed to Azure Container Apps using the included GitHub Actions workflow:

1. Set up the following GitHub secrets in your repository with your personal API credentials:
   - `AZURE_CREDENTIALS`: Azure service principal credentials
   - `DOCKERHUB_USERNAME`: Docker Hub username
   - `DOCKERHUB_TOKEN`: Docker Hub access token
   - `AZURE_SPEECH_KEY`: Your Azure Speech API key
   - `AZURE_SPEECH_REGION`: Your Azure region
   - `GEMINI_API_KEY`: Your Google Gemini API key
   - `PICOVOICE_API_KEY`: Your Picovoice API key

2. The workflow will automatically:
   - Build the Docker image for the backend
   - Push it to Docker Hub
   - Deploy to Azure Container Apps
   - Configure environment variables
   - Set up health monitoring

For manual deployment to Azure:

1. Build the Docker image
2. Push to your container registry
3. Deploy to Azure Container Apps with appropriate environment variables

### Mobile App Deployment

The Flutter application can be built for various platforms:

- **Android**: `flutter build apk`
- **iOS**: `flutter build ios`
- **Web**: `flutter build web`

## ğŸ“¡ API Endpoints

The server exposes these main endpoints:

- `POST /api/conversation`: Translates text with comprehensive metadata
- `POST /api/speech-to-text`: Converts audio file to text
- `POST /api/voice-command`: Processes wake word commands
- `GET /api/audio/{filename}`: Retrieves generated audio files

## ğŸ”® Future Enhancements

- Additional language support beyond English, German, and Spanish
- Offline translation capabilities
- Custom voice and pronunciation options
- Expanded grammar explanation features
- Conversation history export
- Integration with language learning platforms
- Enhanced vocabulary building tools
- Customizable wake words

---

<a id="deutsch"></a>

# ğŸ—£ï¸ Speak and Translate - KI-Ãœbersetzungsassistent [Deutsch]

Eine plattformÃ¼bergreifende Anwendung, entwickelt mit Flutter und Azure KI-Diensten, die Echtzeit-SprachÃ¼bersetzung, Sprachbefehle und natÃ¼rliche Sprachsynthese bietet. Die App unterstÃ¼tzt GesprÃ¤che zwischen mehreren Sprachen mit umfangreichen Lernfunktionen fÃ¼r SprachschÃ¼ler.

## ğŸ“‹ Funktionen

- **ğŸ™ï¸ Sprachbefehle**: Aktivieren Sie das Mikrofon mit "Jarvis" und stoppen Sie die Aufnahme mit "Alexa"
- **ğŸ”Š Spracherkennung**: Konvertieren Sie gesprochene Sprache mit hoher Genauigkeit in Text
- **ğŸŒ Echtzeit-Ãœbersetzung**: Ãœbersetzen Sie zwischen Sprachen mit KI-gestÃ¼tzter PrÃ¤zision
- **ğŸ—£ï¸ NatÃ¼rliche Sprachsynthese**: HÃ¶ren Sie Ãœbersetzungen in natÃ¼rlich klingenden Stimmen
- **ğŸ§  Lehrreiche Ãœbersetzungen**: Sehen Sie Wort-fÃ¼r-Wort-Ãœbersetzungen zum Sprachenlernen
- **ğŸ­ Mehrere Ãœbersetzungsstile**: Zugriff auf muttersprachliche, umgangssprachliche, informelle und formelle Ãœbersetzungen
- **ğŸ’¬ Chat-OberflÃ¤che**: Verfolgen Sie den GesprÃ¤chsverlauf mit visuellen Sprechblasen
- **ğŸ›ï¸ Freisprechmodus**: Spielen Sie Ãœbersetzungen automatisch fÃ¼r fortlaufende GesprÃ¤che ab
- **ğŸ”„ Zweisprachige Audiowiedergabe**: HÃ¶ren Sie Quell- und Ã¼bersetzte WÃ¶rter nacheinander
- **ğŸ“± PlattformÃ¼bergreifend**: Funktioniert auf Android, iOS und Web-Plattformen

### ÃœbersetzungsfÃ¤higkeiten im Detail

Die Anwendung bietet fortschrittliche Ãœbersetzungsoptionen mit vier verschiedenen FormalitÃ¤tsstufen:

1. **Muttersprachlich**: Professionelle Ãœbersetzung, wie ein Muttersprachler sie ausdrÃ¼cken wÃ¼rde
2. **Umgangssprachlich**: LÃ¤ssige, alltÃ¤gliche Sprache mit Redewendungen und AusdrÃ¼cken
3. **Informell**: Entspannte Sprache fÃ¼r lockere Situationen
4. **Formell**: Professionelle Sprache fÃ¼r geschÃ¤ftliche oder formelle Kontexte

Jede Ãœbersetzung enthÃ¤lt Wort-fÃ¼r-Wort-ErklÃ¤rungen, um Benutzern zu helfen, die Struktur und den Wortschatz verschiedener Sprachen zu verstehen, was diese Anwendung besonders wertvoll fÃ¼r Sprachenlerner macht.

## ğŸ”§ Technische Umsetzung

### ArchitekturÃ¼berblick

Die Anwendung besteht aus zwei Hauptkomponenten:

1. **Flutter Mobile Client**:
   - BenutzeroberflÃ¤che fÃ¼r Benutzerinteraktion und Chat-Schnittstelle
   - Verarbeitet Audioaufnahme und -wiedergabe
   - Verwaltet GesprÃ¤chsverlauf

2. **Python Backend Server**:
   - FastAPI REST-Dienst fÃ¼r Ãœbersetzung und Sprachverarbeitung
   - Integriert Googles Gemini KI fÃ¼r fortschrittliche Ãœbersetzung
   - Verwendet Azure Cognitive Services fÃ¼r Sprachsynthese und -erkennung
   - Containerisiert mit Docker fÃ¼r einfache Bereitstellung

### Verwendete Technologien

#### Mobile App
- Flutter mit Dart
- Riverpod fÃ¼r Zustandsverwaltung
- Audioaufnahme- und Wiedergabebibliotheken
- Material Design UI-Komponenten

#### Backend Server
- Python 3.9
- FastAPI Web-Framework
- Azure Cognitive Services Speech SDK
- Googles Gemini 2.0 KI-Modell
- Docker fÃ¼r Containerisierung
- Pydub fÃ¼r Audioverarbeitung

#### DevOps
- GitHub Actions fÃ¼r CI/CD
- Azure Container Apps fÃ¼r Hosting
- Docker Hub fÃ¼r Container-Registry

### Code-Organisation

Der Quellcode ist nach Clean-Architecture-Prinzipien strukturiert:

```
â”œâ”€â”€ .github/workflows/      # CI/CD-Konfiguration fÃ¼r Azure-Bereitstellung
â”œâ”€â”€ lib/                    # Flutter-Anwendungsquellcode
â”‚   â”œâ”€â”€ features/           # Funktionsmodule (Ãœbersetzung, Stimme, usw.)
â”‚   â”‚   â””â”€â”€ translation/    # Ãœbersetzungsfunktion
â”‚   â”‚       â”œâ”€â”€ data/       # Datenschicht mit Modellen und Repositories
â”‚   â”‚       â”œâ”€â”€ domain/     # DomÃ¤nenschicht mit EntitÃ¤ten und AnwendungsfÃ¤llen
â”‚   â”‚       â””â”€â”€ presentation/ # UI-Schicht mit Bildschirmen und Providern
â”‚   â””â”€â”€ main.dart           # Flutter-Anwendungseinstiegspunkt
â””â”€â”€ server/                 # Backend-Server
    â”œâ”€â”€ app/                # Server-Anwendungscode
    â”‚   â”œâ”€â”€ application/    # Anwendungsdienste
    â”‚   â”œâ”€â”€ domain/         # DomÃ¤nenentitÃ¤ten
    â”‚   â””â”€â”€ infrastructure/ # API-Routen und externe Schnittstellen
    â”œâ”€â”€ Dockerfile          # Docker-Konfiguration fÃ¼r Server
    â””â”€â”€ requirements.txt    # Python-AbhÃ¤ngigkeiten
```

## ğŸš€ Erste Schritte

### Voraussetzungen

Bevor Sie beginnen, benÃ¶tigen Sie:

1. Flutter SDK (neueste Version)
2. Docker (fÃ¼r Backend)
3. Folgende API-Anmeldedaten:
   - **Azure Speech Services**: Erstellen Sie ein Konto im [Azure Portal](https://portal.azure.com), erstellen Sie eine Speech-Service-Ressource und holen Sie Ihren API-SchlÃ¼ssel und Ihre Region
   - **Google Gemini API**: Melden Sie sich bei [Google AI Studio](https://makersuite.google.com/app/apikey) an, um Ihren Gemini API-SchlÃ¼ssel zu erhalten
   - **Picovoice API**: Erstellen Sie ein Konto bei [Picovoice Console](https://console.picovoice.ai/), um Ihren API-SchlÃ¼ssel fÃ¼r die Aktivierungswort-Erkennung zu erhalten

### Projekt von Grund auf erstellen

1. Erstellen Sie ein neues Verzeichnis fÃ¼r Ihr Projekt:
```bash
# FÃ¼r Windows
mkdir speak_and_translate_app
cd speak_and_translate_app

# FÃ¼r macOS/Linux
mkdir speak_and_translate_app
cd speak_and_translate_app
```

2. Erstellen Sie ein neues Flutter-Projekt:
```bash
flutter create speak_and_translate
cd speak_and_translate
```

3. Ersetzen Sie die Standard-Flutter-Projektstruktur durch die Repository-Struktur:
   - Ersetzen Sie den Inhalt des `lib`-Ordners durch den `lib`-Ordner des Repositories
   - Erstellen Sie einen `server`-Ordner und fÃ¼gen Sie die Backend-Dateien hinzu

### Server-Einrichtung

1. Erstellen Sie ein `server`-Verzeichnis im Projektstamm und navigieren Sie dorthin:
```bash
mkdir server
cd server
```

2. Kopieren Sie die Server-Dateien aus dem Repository oder erstellen Sie die notwendigen Dateien manuell gemÃ¤ÃŸ der Repository-Struktur.

3. Richten Sie Umgebungsvariablen ein:
   Erstellen Sie eine `.env`-Datei im Server-Verzeichnis mit Ihren eigenen API-Anmeldedaten:
```
AZURE_SPEECH_KEY=ihr_azure_speech_key
AZURE_SPEECH_REGION=ihre_azure_region
GEMINI_API_KEY=ihr_gemini_api_key
PICOVOICE_API_KEY=ihr_picovoice_api_key
TTS_DEVICE=cpu
```

4. FÃ¼hren Sie mit Docker aus:
```bash
docker-compose up -d
```

5. Oder fÃ¼hren Sie lokal mit Python aus:
```bash
pip install -r requirements.txt
python -m app.main
```

### Mobile App-Einrichtung

1. Navigieren Sie zurÃ¼ck zum Mobile-App-Verzeichnis:
```bash
cd ..
```

2. Aktualisieren Sie die `pubspec.yaml`-Datei mit den erforderlichen AbhÃ¤ngigkeiten:
```yaml
dependencies:
  flutter:
    sdk: flutter
  cupertino_icons: ^1.0.8
  flutter_riverpod: ^2.6.1
  http: ^1.2.2
  json_annotation: ^4.9.0
  path_provider: ^2.1.5
  record: ^5.2.0
  permission_handler: ^11.3.1
  record_platform_interface: ^1.2.0
  flutter_sound: ^9.19.1
  just_audio: ^0.9.43
  flutter_markdown: ^0.7.5
  flutter_tts: ^4.2.1
  picovoice_flutter: ^3.0.4
  speech_to_text: ^7.0.0
```

3. Installieren Sie AbhÃ¤ngigkeiten:
```bash
flutter pub get
```

4. Konfigurieren Sie den API-Endpunkt:
   In `lib/features/translation/data/repositories/translation_repository_impl.dart` aktualisieren Sie die `baseUrl`, um auf Ihren Server zu zeigen:
```dart
// Ã„ndern Sie diese Zeile, um auf Ihren Server zu zeigen
final String baseUrl = 'http://localhost:8000';
```

5. Richten Sie Berechtigungen ein:
   - FÃ¼r Android: FÃ¼gen Sie Mikrofonberechtigungen zu `android/app/src/main/AndroidManifest.xml` hinzu:
   ```xml
   <uses-permission android:name="android.permission.RECORD_AUDIO"/>
   <uses-permission android:name="android.permission.INTERNET"/>
   ```
   
   - FÃ¼r iOS: FÃ¼gen Sie Mikrofonberechtigungen zu `ios/Runner/Info.plist` hinzu:
   ```xml
   <key>NSMicrophoneUsageDescription</key>
   <string>Diese App benÃ¶tigt Mikrofonzugriff fÃ¼r Spracherkennung</string>
   ```

6. FÃ¼hren Sie die App aus:
```bash
flutter run
```

## ğŸ“¦ Bereitstellung

### API-Anmeldedaten erhalten

Vor der Bereitstellung mÃ¼ssen Sie Ihre eigenen API-Anmeldedaten erhalten:

1. **Azure Speech Services**:
   - Gehen Sie zum [Azure Portal](https://portal.azure.com)
   - Erstellen Sie eine neue Ressource â†’ KI + Machine Learning â†’ Speech Service
   - Nach der Erstellung gehen Sie zu "SchlÃ¼ssel und Endpunkt", um Ihren SchlÃ¼ssel und Ihre Region zu finden

2. **Google Gemini API**:
   - Besuchen Sie [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Erstellen Sie einen API-SchlÃ¼ssel fÃ¼r das Gemini-Modell

3. **Picovoice API**:
   - Erstellen Sie ein Konto bei [Picovoice Console](https://console.picovoice.ai/)
   - Generieren Sie einen neuen ZugriffsschlÃ¼ssel fÃ¼r die Aktivierungswort-Erkennung

### Server-Bereitstellung mit Azure Container Apps

Das Backend kann mit dem enthaltenen GitHub Actions-Workflow auf Azure Container Apps bereitgestellt werden:

1. Richten Sie die folgenden GitHub-Secrets in Ihrem Repository mit Ihren persÃ¶nlichen API-Anmeldedaten ein:
   - `AZURE_CREDENTIALS`: Azure-Dienstprinzipal-Anmeldeinformationen
   - `DOCKERHUB_USERNAME`: Docker Hub-Benutzername
   - `DOCKERHUB_TOKEN`: Docker Hub-Zugriffstoken
   - `AZURE_SPEECH_KEY`: Ihr Azure Speech API-SchlÃ¼ssel
   - `AZURE_SPEECH_REGION`: Ihre Azure-Region
   - `GEMINI_API_KEY`: Ihr Google Gemini API-SchlÃ¼ssel
   - `PICOVOICE_API_KEY`: Ihr Picovoice API-SchlÃ¼ssel

2. Der Workflow wird automatisch:
   - Das Docker-Image fÃ¼r das Backend erstellen
   - Es auf Docker Hub pushen
   - Auf Azure Container Apps bereitstellen
   - Umgebungsvariablen konfigurieren
   - GesundheitsÃ¼berwachung einrichten

FÃ¼r manuelle Bereitstellung auf Azure:

1. Erstellen Sie das Docker-Image
2. Pushen Sie es in Ihre Container-Registry
3. Stellen Sie es auf Azure Container Apps mit entsprechenden Umgebungsvariablen bereit

### Mobile App-Bereitstellung

Die Flutter-Anwendung kann fÃ¼r verschiedene Plattformen erstellt werden:

- **Android**: `flutter build apk`
- **iOS**: `flutter build ios`
- **Web**: `flutter build web`

## ğŸ“¡ API-Endpunkte

Der Server stellt diese Haupt-Endpunkte bereit:

- `POST /api/conversation`: Ãœbersetzt Text mit umfassenden Metadaten
- `POST /api/speech-to-text`: Konvertiert Audiodatei zu Text
- `POST /api/voice-command`: Verarbeitet Aktivierungswort-Befehle
- `GET /api/audio/{filename}`: Ruft generierte Audiodateien ab

## ğŸ”® ZukÃ¼nftige Erweiterungen

- ZusÃ¤tzliche SprachunterstÃ¼tzung Ã¼ber Englisch, Deutsch und Spanisch hinaus
- Offline-ÃœbersetzungsfÃ¤higkeiten
- Benutzerdefinierte Stimm- und Ausspracheoptionen
- Erweiterte GrammatikerklÃ¤rungsfunktionen
- Export des GesprÃ¤chsverlaufs
- Integration mit Sprachlernplattformen
- Verbesserte Werkzeuge zum Wortschatzaufbau
- Anpassbare AktivierungswÃ¶rter
